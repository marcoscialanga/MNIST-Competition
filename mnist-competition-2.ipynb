{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is my first time working with Neural Networks. As a beginner, I learned a lot from other notebooks on Kaggle, especially Yassine Ghouzam's."},{"metadata":{},"cell_type":"markdown","source":"Let's start by importing the libraries we need. To build our neural network, we'll use keras."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n%matplotlib inline\n\nnp.random.seed(2)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's import the data. We already have a dataset for training and one for testing."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\") #labelled\ntest_data = pd.read_csv(\"/kaggle/input/digit-recognizer/test.csv\") #not labelled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for missing values\ntrain_data.isna().any().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.isnull().any().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_y = train_data[\"label\"]\ntrain_x = train_data.drop(labels = \"label\", axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We notice there are no missing values. So we can proceed."},{"metadata":{},"cell_type":"markdown","source":"Now we normalize the data. The model will work better if the values are between 0 and 1. From the description of the dataset, we know that: \"Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.\" Thus, if we divide our values by 255, proportions will hold but all values will be between 0 and 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x = train_x / 255\ntest_data = test_data / 255","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The images are stored as 1D lists of 784 values. We notice that 784 = 28^2. Thus, we can reshape our data into 28 * 28 images."},{"metadata":{"trusted":true},"cell_type":"code","source":"#reshape data\ntrain_x = train_x.values.reshape(-1,28,28,1)\ntest_data = test_data.values.reshape(-1,28,28,1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Encode labels (1,2,3 etc.) to one-hot vectors. For example, 1 becomes 0100000000"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_y = to_categorical(train_y, num_classes = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_seed = 2\ntrain_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size = 0.1, random_state = random_seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how our digits look like, as 28 * 28 pixel images."},{"metadata":{"trusted":true},"cell_type":"code","source":"g = plt.imshow(train_x[0][:,:,0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's build the model. I learned this from Yassine Ghouzam's notebook."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2))) #reduce computational cost, picks maximum value between two neighbourhing pixels\nmodel.add(Dropout(0.25)) #reduce overfitting by ignoring some nodes randomly\n\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Set up optimizer to improve parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\nmodel.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 1 #only one epoch to fit the model quickly\nbatch_size = 86 #86 digits passed each time through the model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(train_x, train_y, batch_size = batch_size, epochs = epochs, \n                    validation_data = (val_x, val_y), verbose = 2) \n\n#verbose is just a parameter to decide how the fitting is proceeding","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can see our results on the validation data. Since we have achieved a pretty good accuracy without training the model for too long, we can run the model on the test data and submit our results."},{"metadata":{"trusted":true},"cell_type":"code","source":"results = model.predict(test_data)\n\nresults = np.argmax(results,axis = 1)\n\nresults = pd.Series(results,name=\"Label\")\n\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"cnn_mnist_datagen.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We achieved about 97.37 accuracy."}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}